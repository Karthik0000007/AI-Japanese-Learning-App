# ğŸˆº English â‡„ Japanese Smart Translator
### A Real-Time Speech, Text & Furigana Translation App

---

## ğŸš€ Overview

This project is a fully functional real-time **English â‡„ Japanese translation tool**, integrating:

- ğŸ” English â‡„ Japanese translation (with polite/casual/honorific tone control)
- ğŸ’¬ Furigana (ruby) annotations for kanji
- ğŸ™ï¸ Text-to-Speech using gTTS
- ğŸ§  **Grammar/Kanji/JLPT explanation using LLaMA3 via Ollama**
- ğŸ¤ Speech-to-text using browser APIs

> The project evolved through multiple versions â€” each iteration improving the features and capabilities.
>  
> Youâ€™ll find multiple files showing the step-by-step growth of both the frontend and backend.

---
## ğŸ¤– LLaMA3 + Ollama Integration

This project uses the [Ollama](https://ollama.com) runtime to locally run the [Meta LLaMA3](https://ai.meta.com/llama/) model. The assistant is accessed via an endpoint `/japanese_helper`, which returns:
- Vocabulary breakdown with furigana and meanings
- Grammar explanations
- JLPT levels
- Kanji readings and meanings

---
> ğŸ’¡ This allows offline, secure, and highly contextual Japanese language tutoring directly from an LLM.

## ğŸ“‚ Project Structure

```bash
root/
â”‚
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ index.html         # Initial basic UI version
â”‚   â”œâ”€â”€ index2.html         # Added speech input
â”‚   â”œâ”€â”€ index3.html         # Improved UI design
â”‚   â”œâ”€â”€ index4.html         # Added TTS functionality
â”‚   â”œâ”€â”€ index5.html         # Integrated Furigana display
â”‚   â”œâ”€â”€ main.html           
|   â”œâ”€â”€ llama.html       # Final complete version. Main file to run
â”‚
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app.py           # Initial Flask API with basic translation
â”‚   â”œâ”€â”€ app2.py           # Added speech-to-text backend
â”‚   â”œâ”€â”€ app3.py           # Integrated mBART50 model
â”‚   â”œâ”€â”€ app4.py           # Added Furigana processing using Fugashi
â”‚   â”œâ”€â”€ app5.py           # Improved error handling and optimizations
â”‚   â”œâ”€â”€ main.py
|   â”œâ”€â”€ llama.py      # Added Meta LLaMA3 via Ollama for JLPT grammar assistant. Main file to run
â”‚
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # (This file)

```

## ğŸ”¥ Features
- âœ… Real-time Speech-to-Text (STT) for English input
- âœ… Text input for manual typing
- âœ… Real-time mBART50 powered translation (English â‡„ Japanese)
- âœ… Politeness adjustments for Japanese output
- âœ… Automatic Furigana generation (word-level kana over kanji)
- âœ… Listen to Japanese translation with Text-to-Speech
- âœ… Fully functional responsive frontend

## ğŸ› ï¸ Technologies Used
- Frontend: HTML5, Vanilla JS, Web Speech API, Fetch API
- Backend: Python, Flask, Flask-CORS
- Machine Learning Model: Facebook mBART50 (via Hugging Face Transformers)
- Japanese NLP: MeCab, Fugashi, jaconv
- Speech Recognition: Browser native (Web Speech API)
- Text-To-Speech: Google gTTS
- Deployment Ready: Fully compatible with cloud platforms

## âš™ï¸ Setup Instructions
- 1ï¸âƒ£ Install dependencies
Create a virtual environment (optional but recommended):
python -m venv venv
venv\Scripts\activate     # (Windows)

- Install requirements:
pip install -r requirements.txt

#### Run the LLaMA3 Model
Make sure you have [Ollama installed](https://ollama.com/download):

```bash
ollama run llama3
```

- 2ï¸âƒ£ Run the Backend
From Backend/ directory:
python llama.py

- 3ï¸âƒ£ Run the Frontend
Simply open Frontend/llama.html in your browser.

- âœ… Both frontend & backend will communicate via REST API locally.

## ğŸš€ Future Work 
- Fine-tune mBART model for domain-specific translation.
- Deploy on cloud platforms (Render, HuggingFace, AWS, etc.)
- Add user authentication and history storage.
- Improve furigana rendering using ruby tags for perfect web display.
- Expand language pairs beyond English-Japanese.

## ğŸ§‘â€ğŸ’» Author
- This project was built as part of my personal language learning + AI research journey.

## ğŸ’¡ Special Notes
- The multiple versions inside both Frontend and Backend folders represent the evolution of the project, showcasing how features were gradually added and optimized over time.
